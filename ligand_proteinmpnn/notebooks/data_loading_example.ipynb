{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0157fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse PDB files into a dictionary\n",
    "# use parse_multiple_chains.py; parse_multiple_chains.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36de0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60d4080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully finished: 2 pdbs\n"
     ]
    }
   ],
   "source": [
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import os, time, gzip, json\n",
    "import glob \n",
    "import argparse\n",
    "\n",
    "# argparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# argparser.add_argument(\"--pdb_folder\", type=str, help=\"Path to a folder with pdb files, e.g. /home/my_pdbs/\")\n",
    "# argparser.add_argument(\"--out_path\", type=str, help=\"Path where to save .jsonl dictionary of parsed pdbs\")\n",
    "\n",
    "# args = argparser.parse_args()\n",
    "\n",
    "# folder_with_pdbs_path = args.pdb_folder\n",
    "# save_path = args.out_path\n",
    "\n",
    "#MODIFY THIS PATH TO YOUR FOLDER WITH PDBS\n",
    "folder_with_pdbs_path = '/home/justas/projects/lab_github/mpnn/pdbs'\n",
    "#MODIFY OUTPUT PATH\n",
    "save_path = '/home/justas/projects/lab_github/mpnn/data/pdbs.jsonl'\n",
    "\n",
    "alpha_1 = list(\"ARNDCQEGHILKMFPSTWYV-\")\n",
    "states = len(alpha_1)\n",
    "alpha_3 = ['ALA','ARG','ASN','ASP','CYS','GLN','GLU','GLY','HIS','ILE',\n",
    "           'LEU','LYS','MET','PHE','PRO','SER','THR','TRP','TYR','VAL','GAP']\n",
    "\n",
    "aa_1_N = {a:n for n,a in enumerate(alpha_1)}\n",
    "aa_3_N = {a:n for n,a in enumerate(alpha_3)}\n",
    "aa_N_1 = {n:a for n,a in enumerate(alpha_1)}\n",
    "aa_1_3 = {a:b for a,b in zip(alpha_1,alpha_3)}\n",
    "aa_3_1 = {b:a for a,b in zip(alpha_1,alpha_3)}\n",
    "\n",
    "def AA_to_N(x):\n",
    "  # [\"ARND\"] -> [[0,1,2,3]]\n",
    "  x = np.array(x);\n",
    "  if x.ndim == 0: x = x[None]\n",
    "  return [[aa_1_N.get(a, states-1) for a in y] for y in x]\n",
    "\n",
    "def N_to_AA(x):\n",
    "  # [[0,1,2,3]] -> [\"ARND\"]\n",
    "  x = np.array(x);\n",
    "  if x.ndim == 1: x = x[None]\n",
    "  return [\"\".join([aa_N_1.get(a,\"-\") for a in y]) for y in x]\n",
    "\n",
    "\n",
    "def parse_PDB_biounits(x, atoms=['N','CA','C'], chain=None):\n",
    "  '''\n",
    "  input:  x = PDB filename\n",
    "          atoms = atoms to extract (optional)\n",
    "  output: (length, atoms, coords=(x,y,z)), sequence\n",
    "  '''\n",
    "  xyz,seq,min_resn,max_resn = {},{},1e6,-1e6\n",
    "  for line in open(x,\"rb\"):\n",
    "    line = line.decode(\"utf-8\",\"ignore\").rstrip()\n",
    "\n",
    "    if line[:6] == \"HETATM\" and line[17:17+3] == \"MSE\":\n",
    "      line = line.replace(\"HETATM\",\"ATOM  \")\n",
    "      line = line.replace(\"MSE\",\"MET\")\n",
    "\n",
    "    if line[:4] == \"ATOM\":\n",
    "      ch = line[21:22]\n",
    "      if ch == chain or chain is None:\n",
    "        atom = line[12:12+4].strip()\n",
    "        resi = line[17:17+3]\n",
    "        resn = line[22:22+5].strip()\n",
    "        x,y,z = [float(line[i:(i+8)]) for i in [30,38,46]]\n",
    "\n",
    "        if resn[-1].isalpha(): \n",
    "            resa,resn = resn[-1],int(resn[:-1])-1\n",
    "        else: \n",
    "            resa,resn = \"\",int(resn)-1\n",
    "#         resn = int(resn)\n",
    "        if resn < min_resn: \n",
    "            min_resn = resn\n",
    "        if resn > max_resn: \n",
    "            max_resn = resn\n",
    "        if resn not in xyz: \n",
    "            xyz[resn] = {}\n",
    "        if resa not in xyz[resn]: \n",
    "            xyz[resn][resa] = {}\n",
    "        if resn not in seq: \n",
    "            seq[resn] = {}\n",
    "        if resa not in seq[resn]: \n",
    "            seq[resn][resa] = resi\n",
    "\n",
    "        if atom not in xyz[resn][resa]:\n",
    "          xyz[resn][resa][atom] = np.array([x,y,z])\n",
    "\n",
    "  # convert to numpy arrays, fill in missing values\n",
    "  seq_,xyz_ = [],[]\n",
    "  try:\n",
    "      for resn in range(min_resn,max_resn+1):\n",
    "        if resn in seq:\n",
    "          for k in sorted(seq[resn]): seq_.append(aa_3_N.get(seq[resn][k],20))\n",
    "        else: seq_.append(20)\n",
    "        if resn in xyz:\n",
    "          for k in sorted(xyz[resn]):\n",
    "            for atom in atoms:\n",
    "              if atom in xyz[resn][k]: xyz_.append(xyz[resn][k][atom])\n",
    "              else: xyz_.append(np.full(3,np.nan))\n",
    "        else:\n",
    "          for atom in atoms: xyz_.append(np.full(3,np.nan))\n",
    "      return np.array(xyz_).reshape(-1,len(atoms),3), N_to_AA(np.array(seq_))\n",
    "  except TypeError:\n",
    "      return 'no_chain', 'no_chain'\n",
    "\n",
    "\n",
    "\n",
    "pdb_dict_list = []\n",
    "c = 0\n",
    "\n",
    "if folder_with_pdbs_path[-1]!='/':\n",
    "    folder_with_pdbs_path = folder_with_pdbs_path+'/'\n",
    "\n",
    "\n",
    "init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
    "extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
    "chain_alphabet = init_alphabet + extra_alphabet\n",
    "\n",
    "biounit_names = glob.glob(folder_with_pdbs_path+'*.pdb')\n",
    "for biounit in biounit_names:\n",
    "    my_dict = {}\n",
    "    s = 0\n",
    "    concat_seq = ''\n",
    "    concat_N = []\n",
    "    concat_CA = []\n",
    "    concat_C = []\n",
    "    concat_O = []\n",
    "    concat_mask = []\n",
    "    coords_dict = {}\n",
    "    for letter in chain_alphabet:\n",
    "        xyz, seq = parse_PDB_biounits(biounit, atoms=['N','CA','C','O'], chain=letter)\n",
    "        if type(xyz) != str:\n",
    "            concat_seq += seq[0]\n",
    "            my_dict['seq_chain_'+letter]=seq[0]\n",
    "            coords_dict_chain = {}\n",
    "            coords_dict_chain['N_chain_'+letter]=xyz[:,0,:].tolist()\n",
    "            coords_dict_chain['CA_chain_'+letter]=xyz[:,1,:].tolist()\n",
    "            coords_dict_chain['C_chain_'+letter]=xyz[:,2,:].tolist()\n",
    "            coords_dict_chain['O_chain_'+letter]=xyz[:,3,:].tolist()\n",
    "            my_dict['coords_chain_'+letter]=coords_dict_chain\n",
    "            s += 1\n",
    "    fi = biounit.rfind(\"/\")\n",
    "    my_dict['name']=biounit[(fi+1):-4]\n",
    "    my_dict['num_of_chains'] = s\n",
    "    my_dict['seq'] = concat_seq\n",
    "    if s < len(chain_alphabet):\n",
    "        pdb_dict_list.append(my_dict)\n",
    "        c+=1\n",
    "        \n",
    "        \n",
    "with open(save_path, 'w') as f:\n",
    "    for entry in pdb_dict_list:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "        \n",
    "print(f'Successfully finished: {len(biounit_names)} pdbs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0704e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE masked/visible chains dictionary\n",
    "#Masked means the chains that need to be designed, e.g. binder\n",
    "#Visible means the chains that are fixed, they won't be modelled but only used as a context, e.g. target\n",
    "# use make_masked_visible_chain_dict.py; make_masked_visible_chain_dict.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71b97ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "#MODIFY THIS PATH - it is a path to the parsed pdb files\n",
    "with open('/home/justas/projects/lab_github/mpnn/data/pdbs.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "my_dict = {}\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    all_chain_list = [item[-1:] for item in list(result) if item[:9]=='seq_chain'] #['A','B', 'C',...]\n",
    "    masked_chain_list = ['A'] #predict sequence of chain A\n",
    "    visible_chain_list = ['B'] #allow to use chain B as a context\n",
    "    my_dict[result['name']]= (masked_chain_list, visible_chain_list)\n",
    "\n",
    "#MODIFY THIS PATH FOR THE OUTPUT DICTIONARY\n",
    "with open('/home/justas/projects/lab_github/mpnn/data/pdbs_masked.jsonl', 'w') as f:\n",
    "    f.write(json.dumps(my_dict) + '\\n')\n",
    "\n",
    "\n",
    "print('Finished')\n",
    "# Output looks like this:\n",
    "# {\"5TTA\": [[\"A\"], [\"B\"]], \"3LIS\": [[\"A\"], [\"B\"]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96908e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX some positions in the designed sequence, e.g. fixing interface residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ac3d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20fb0b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 7, 8, 9, 22, 25, 33]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#itertools example to make a list\n",
    "list(itertools.chain(list(np.arange(1,4)), list(np.arange(7,10)), [22, 25, 33]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27b5ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX some positions\n",
    "#use make_fixed_position_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b28d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5TTA\n",
      "3LIS\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "#MODIFY this path\n",
    "with open('/home/justas/projects/lab_github/mpnn/data/pdbs.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "my_dict = {}\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    all_chain_list = [item[-1:] for item in list(result) if item[:9]=='seq_chain']\n",
    "    fixed_position_dict = {}\n",
    "    print(result['name'])\n",
    "    #FIX ONLY PDB NAMED 5TTA in the chain A\n",
    "    if result['name'] == '5TTA':\n",
    "        for chain in all_chain_list:\n",
    "            if chain == 'A':\n",
    "                fixed_position_dict[chain] = [int(item) for item in list(itertools.chain(list(np.arange(1,4)), list(np.arange(7,10)), [22, 25, 33]))]\n",
    "            else:\n",
    "                fixed_position_dict[chain] = []\n",
    "    else:\n",
    "        for chain in all_chain_list:\n",
    "            fixed_position_dict[chain] = []\n",
    "    my_dict[result['name']] = fixed_position_dict\n",
    "\n",
    "#MODIFY this path   \n",
    "with open('/home/justas/projects/lab_github/mpnn/data/pdbs_fixed.jsonl', 'w') as f:\n",
    "    f.write(json.dumps(my_dict) + '\\n')\n",
    "\n",
    "\n",
    "print('Finished')\n",
    "#e.g. output\n",
    "#{\"5TTA\": {\"A\": [1, 2, 3, 7, 8, 9, 22, 25, 33], \"B\": []}, \"3LIS\": {\"A\": [], \"B\": []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b358f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tie positions together\n",
    "#In the example below homo-dimer symmetry is created, {\"A\": [1], \"B\": [1]}, {\"A\": [2], \"B\": [2]}, {\"A\": [3], \"B\": [3]}...\n",
    "# use make_tied_positions_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a66406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "#MODIFY this path\n",
    "with open('/home/justas/projects/lab_github/mpnn/data/pdbs.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "my_dict = {}\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    all_chain_list = sorted([item[-1:] for item in list(result) if item[:9]=='seq_chain']) #A, B, C, ...\n",
    "    tied_positions_list = []\n",
    "    if result['name'] == '3LIS':\n",
    "        chain_length = len(result[\"seq_chain_A\"])\n",
    "        for i in range(1,chain_length+1):\n",
    "            temp_dict = {}\n",
    "            temp_dict[all_chain_list[0]] = [i]  #all_chain_list[0] == \"A\" in this case\n",
    "            temp_dict[all_chain_list[1]] = [i]  #all_chain_list[0] == \"B\"\n",
    "            tied_positions_list.append(temp_dict)\n",
    "    else:\n",
    "        tied_positions_list = []\n",
    "    my_dict[result['name']] = tied_positions_list\n",
    "\n",
    "#Write output to:    \n",
    "with open('/home/justas/projects/lab_github/mpnn/data/pdbs_tied.jsonl', 'w') as f:\n",
    "    f.write(json.dumps(my_dict) + '\\n')\n",
    "\n",
    "\n",
    "print('Finished')\n",
    "\n",
    "#e.g. output\n",
    "#{\"5TTA\": [], \"3LIS\": [{\"A\": [1], \"B\": [1]}, {\"A\": [2], \"B\": [2]}, {\"A\": [3], \"B\": [3]}, {\"A\": [4], \"B\": [4]}, {\"A\": [5], \"B\": [5]}, {\"A\": [6], \"B\": [6]}, {\"A\": [7], \"B\": [7]}, {\"A\": [8], \"B\": [8]}, {\"A\": [9], \"B\": [9]}, {\"A\": [10], \"B\": [10]}, {\"A\": [11], \"B\": [11]}, {\"A\": [12], \"B\": [12]}, {\"A\": [13], \"B\": [13]}, {\"A\": [14], \"B\": [14]}, {\"A\": [15], \"B\": [15]}, {\"A\": [16], \"B\": [16]}, {\"A\": [17], \"B\": [17]}, {\"A\": [18], \"B\": [18]}, {\"A\": [19], \"B\": [19]}, {\"A\": [20], \"B\": [20]}, {\"A\": [21], \"B\": [21]}, {\"A\": [22], \"B\": [22]}, {\"A\": [23], \"B\": [23]}, {\"A\": [24], \"B\": [24]}, {\"A\": [25], \"B\": [25]}, {\"A\": [26], \"B\": [26]}, {\"A\": [27], \"B\": [27]}, {\"A\": [28], \"B\": [28]}, {\"A\": [29], \"B\": [29]}, {\"A\": [30], \"B\": [30]}, {\"A\": [31], \"B\": [31]}, {\"A\": [32], \"B\": [32]}, {\"A\": [33], \"B\": [33]}, {\"A\": [34], \"B\": [34]}, {\"A\": [35], \"B\": [35]}, {\"A\": [36], \"B\": [36]}, {\"A\": [37], \"B\": [37]}, {\"A\": [38], \"B\": [38]}, {\"A\": [39], \"B\": [39]}, {\"A\": [40], \"B\": [40]}, {\"A\": [41], \"B\": [41]}, {\"A\": [42], \"B\": [42]}, {\"A\": [43], \"B\": [43]}, {\"A\": [44], \"B\": [44]}, {\"A\": [45], \"B\": [45]}, {\"A\": [46], \"B\": [46]}, {\"A\": [47], \"B\": [47]}, {\"A\": [48], \"B\": [48]}, {\"A\": [49], \"B\": [49]}, {\"A\": [50], \"B\": [50]}, {\"A\": [51], \"B\": [51]}, {\"A\": [52], \"B\": [52]}, {\"A\": [53], \"B\": [53]}, {\"A\": [54], \"B\": [54]}, {\"A\": [55], \"B\": [55]}, {\"A\": [56], \"B\": [56]}, {\"A\": [57], \"B\": [57]}, {\"A\": [58], \"B\": [58]}, {\"A\": [59], \"B\": [59]}, {\"A\": [60], \"B\": [60]}, {\"A\": [61], \"B\": [61]}, {\"A\": [62], \"B\": [62]}, {\"A\": [63], \"B\": [63]}, {\"A\": [64], \"B\": [64]}, {\"A\": [65], \"B\": [65]}, {\"A\": [66], \"B\": [66]}, {\"A\": [67], \"B\": [67]}, {\"A\": [68], \"B\": [68]}, {\"A\": [69], \"B\": [69]}, {\"A\": [70], \"B\": [70]}, {\"A\": [71], \"B\": [71]}, {\"A\": [72], \"B\": [72]}, {\"A\": [73], \"B\": [73]}, {\"A\": [74], \"B\": [74]}, {\"A\": [75], \"B\": [75]}, {\"A\": [76], \"B\": [76]}, {\"A\": [77], \"B\": [77]}, {\"A\": [78], \"B\": [78]}, {\"A\": [79], \"B\": [79]}, {\"A\": [80], \"B\": [80]}, {\"A\": [81], \"B\": [81]}, {\"A\": [82], \"B\": [82]}, {\"A\": [83], \"B\": [83]}, {\"A\": [84], \"B\": [84]}, {\"A\": [85], \"B\": [85]}, {\"A\": [86], \"B\": [86]}, {\"A\": [87], \"B\": [87]}, {\"A\": [88], \"B\": [88]}, {\"A\": [89], \"B\": [89]}, {\"A\": [90], \"B\": [90]}, {\"A\": [91], \"B\": [91]}, {\"A\": [92], \"B\": [92]}, {\"A\": [93], \"B\": [93]}, {\"A\": [94], \"B\": [94]}, {\"A\": [95], \"B\": [95]}, {\"A\": [96], \"B\": [96]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2439e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE global bias dictiobary\n",
    "# use make_bias_AA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0b9e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "my_dict = {\"A\": -0.01, \"G\": 0.02} #0.1 is a good value to start with\n",
    "\n",
    "with open('/home/justas/projects/lab_github/mpnn/data/bias_AA.jsonl', 'w') as f:\n",
    "    f.write(json.dumps(my_dict) + '\\n')\n",
    "    \n",
    "#e.g. output\n",
    "#{\"A\": -0.01, \"G\": 0.02}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb1c5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Omit AAs per position\n",
    "# make_omit_AA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cee30512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5TTA\n",
      "3LIS\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "#MODIFY this path\n",
    "with open('/home/justas/projects/lab_github/mpnn/data/pdbs.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "my_dict = {}\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    all_chain_list = [item[-1:] for item in list(result) if item[:9]=='seq_chain']\n",
    "    fixed_position_dict = {}\n",
    "    print(result['name'])\n",
    "    if result['name'] == '5TTA':\n",
    "        for chain in all_chain_list:\n",
    "            if chain == 'A':\n",
    "                fixed_position_dict[chain] = [\n",
    "                    [[int(item) for item in list(itertools.chain(list(np.arange(1,4)), list(np.arange(7,10)), [22, 25, 33]))], 'GPL'],\n",
    "                    [[int(item) for item in list(itertools.chain([40, 41, 42, 43]))], 'WC'],\n",
    "                    [[int(item) for item in list(itertools.chain(list(np.arange(50,150))))], 'ACEFGHIKLMNRSTVWYX'],\n",
    "                    [[int(item) for item in list(itertools.chain(list(np.arange(160,200))))], 'FGHIKLPQDMNRSTVWYX']]\n",
    "            else:\n",
    "                fixed_position_dict[chain] = []\n",
    "    else:\n",
    "        for chain in all_chain_list:\n",
    "            fixed_position_dict[chain] = []\n",
    "    my_dict[result['name']] = fixed_position_dict\n",
    "\n",
    "#MODIFY this path   \n",
    "with open('/home/justas/projects/lab_github/mpnn/data/omit_AA.jsonl', 'w') as f:\n",
    "    f.write(json.dumps(my_dict) + '\\n')\n",
    "\n",
    "\n",
    "print('Finished')\n",
    "#e.g. output\n",
    "#{\"5TTA\": {\"A\": [[[1, 2, 3, 7, 8, 9, 22, 25, 33], \"GPL\"], [[40, 41, 42, 43], \"WC\"], [[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149], \"ACEFGHIKLMNRSTVWYX\"], [[160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"FGHIKLPQDMNRSTVWYX\"]], \"B\": []}, \"3LIS\": {\"A\": [], \"B\": []}}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make PSSM dictionary to bias MPNN\n",
    "# use make_pssm_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8379723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "def softmax(x, T):\n",
    "    return np.exp(x/T)/np.sum(np.exp(x/T), -1, keepdims=True)\n",
    "\n",
    "def parse_pssm(path):\n",
    "    data = pd.read_csv(path, skiprows=2)\n",
    "    floats_list_list = []\n",
    "    for i in range(data.values.shape[0]):\n",
    "        str1 = data.values[i][0][4:]\n",
    "        floats_list = []\n",
    "        for item in str1.split():\n",
    "            floats_list.append(float(item))\n",
    "        floats_list_list.append(floats_list)\n",
    "    np_lines = np.array(floats_list_list)\n",
    "    return np_lines\n",
    "\n",
    "np_lines = parse_pssm('/home/swang523/RLcage/capsid/monomersfordesign/8-16-21/pssm_rainity_final_8-16-21_int/build_0.2089_0.98_0.4653_19_2.00_0.005745.pssm')\n",
    "\n",
    "mpnn_alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "input_alphabet = 'ARNDCQEGHILKMFPSTWYV'\n",
    "\n",
    "permutation_matrix = np.zeros([20,21])\n",
    "for i in range(20):\n",
    "    letter1 = input_alphabet[i]\n",
    "    for j in range(21):\n",
    "        letter2 = mpnn_alphabet[j]\n",
    "        if letter1 == letter2:\n",
    "            permutation_matrix[i,j]=1.\n",
    "\n",
    "pssm_log_odds = np_lines[:,:20] @ permutation_matrix\n",
    "pssm_probs = np_lines[:,20:40] @ permutation_matrix\n",
    "\n",
    "X_mask = np.concatenate([np.zeros([1,20]), np.ones([1,1])], -1)\n",
    "\n",
    "def softmax(x, T):\n",
    "    return np.exp(x/T)/np.sum(np.exp(x/T), -1, keepdims=True)\n",
    "\n",
    "#Load parsed PDBs:  \n",
    "with open('/home/justas/projects/cages/parsed/test.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "my_dict = {}\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    all_chain_list = [item[-1:] for item in list(result) if item[:9]=='seq_chain']\n",
    "    pssm_dict = {}\n",
    "    for chain in all_chain_list:\n",
    "        pssm_dict[chain] = {}\n",
    "        pssm_dict[chain]['pssm_coef'] = (np.ones(len(result['seq_chain_A']))).tolist() #a number between 0.0 and 1.0 specifying how much attention put to PSSM, can be adjusted later as a flag\n",
    "        pssm_dict[chain]['pssm_bias'] = (softmax(pssm_log_odds-X_mask*1e8, 1.0)).tolist() #PSSM like, [length, 21] such that sum over the last dimension adds up to 1.0\n",
    "        pssm_dict[chain]['pssm_log_odds'] = (pssm_log_odds).tolist()\n",
    "    my_dict[result['name']] = pssm_dict\n",
    "\n",
    "#Write output to:    \n",
    "with open('/home/justas/projects/lab_github/mpnn/data/pssm_dict.jsonl', 'w') as f:\n",
    "    f.write(json.dumps(my_dict) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271fd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
